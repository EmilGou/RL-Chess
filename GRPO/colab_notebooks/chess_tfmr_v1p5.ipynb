{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jwofW7iFLza"
      },
      "outputs": [],
      "source": [
        "!pip install python-chess cairosvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiENzDmTZPnt"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/EmilGou/A-Chess-Transformer/main/uci_moves.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IqpttmRD6kI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJNyG_6-VK-U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBYnrX1aVOMN"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/1.moves\"\n",
        "\n",
        "moves = open(path, \"r\").read()\n",
        "moves = moves.split('\\n\\n')[:-1]\n",
        "GAMES = [m.split('\\n')[:-1] for m in moves]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARDeRH3Aa_G6"
      },
      "outputs": [],
      "source": [
        "from uci_moves import UCI_MOVES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH6J91NME0it"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.svg\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "def uci_moves_to_fen(moves: list[str], show_board: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Given a list of UCI moves, returns the resulting FEN and optionally shows the board.\n",
        "    Args:\n",
        "        moves: List of moves in UCI format, e.g., ['e2e4', 'e7e5']\n",
        "        show_board: If True, display the board with IPython SVG (for notebooks)\n",
        "    Returns:\n",
        "        FEN string of the final board position\n",
        "    \"\"\"\n",
        "    board = chess.Board()\n",
        "    try:\n",
        "        for move in moves:\n",
        "            board.push_uci(move)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"Invalid move '{move}': {e}\")\n",
        "\n",
        "    if show_board:\n",
        "        display(SVG(chess.svg.board(board=board)))\n",
        "\n",
        "    return board.fen()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "didy5BrUHhks"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "\n",
        "UCI_IDS = {v: k for k, v in UCI_MOVES.items()}\n",
        "\n",
        "# 2) BUILD FEN VOCABULARY (covers full FEN)\n",
        "FEN_CHARS = [\n",
        "    '/', ' ', '-',                             # separators & dash\n",
        "    'P','N','B','R','Q','K',\n",
        "    'p','n','b','r','q','k',                   # pieces\n",
        "    '0','1','2','3','4','5','6','7','8','9',    # digits for counters\n",
        "    'a','b','c','d','e','f','g','h',            # files (en passant targets)\n",
        "    'w'\n",
        "]\n",
        "FEN_CHAR_TO_ID = {c: i + len(UCI_MOVES) for i, c in enumerate(FEN_CHARS)}\n",
        "ID_TO_FEN_CHAR = {v: k for k, v in FEN_CHAR_TO_ID.items()}\n",
        "\n",
        "# Compute next available index\n",
        "max_idx = max(FEN_CHAR_TO_ID.values())\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————————————\n",
        "# 3) SPECIAL TOKENS\n",
        "SPECIAL_TOKENS = {\n",
        "    \"<board>\":   max_idx + 1,\n",
        "    \"</board>\":  max_idx + 2,\n",
        "    \"<moves>\":   max_idx + 3,\n",
        "    \"</moves>\":  max_idx + 4,\n",
        "    \"<pad>\":     max_idx + 5,\n",
        "}\n",
        "ID_TO_SPECIAL = {v: k for k, v in SPECIAL_TOKENS.items()}\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————————————\n",
        "# 4) TOKENIZERS / UNTOKENIZER\n",
        "def tokenize_fen(fen: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Turn the full FEN string (all 6 fields) into token IDs,\n",
        "    one per character, dropping only chars not in our vocab.\n",
        "    \"\"\"\n",
        "    return [FEN_CHAR_TO_ID[c] for c in fen if c in FEN_CHAR_TO_ID]\n",
        "\n",
        "def tokenize_uci(moves: list[str]) -> list[int]:\n",
        "    return [UCI_MOVES[m] for m in moves if m in UCI_MOVES]\n",
        "\n",
        "def untokenize(tokens: list[int]) -> list[str]:\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if t in ID_TO_SPECIAL:\n",
        "            out.append(ID_TO_SPECIAL[t])\n",
        "        elif t in ID_TO_FEN_CHAR:\n",
        "            out.append(ID_TO_FEN_CHAR[t])\n",
        "        elif t in UCI_IDS:\n",
        "            out.append(UCI_IDS[t])\n",
        "        else:\n",
        "            out.append(f\"<unk:{t}>\")\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byt_l52YItYe"
      },
      "outputs": [],
      "source": [
        "class ChessGameDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, games: list[list[str]], max_seq_len: int):\n",
        "        self.games = games\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.pad_token = SPECIAL_TOKENS[\"<pad>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.games)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1) split and pick random cut\n",
        "        moves = [m.lower() for m in self.games[idx]]\n",
        "        cutoff = random.randint(1, len(moves) - 1)\n",
        "        past, future = moves[:cutoff], moves[cutoff:]\n",
        "\n",
        "        # 2) build FEN from past\n",
        "        board = chess.Board()\n",
        "        for m in past:\n",
        "            board.push_uci(m)\n",
        "        fen = board.fen()  # full 6-field FEN\n",
        "\n",
        "        # 3) tokenize\n",
        "        fen_tokens    = tokenize_fen(fen)\n",
        "        future_tokens = tokenize_uci(future)\n",
        "\n",
        "        # 4) assemble input + labels\n",
        "        input_seq = (\n",
        "            [SPECIAL_TOKENS[\"<board>\"]] +\n",
        "            fen_tokens +\n",
        "            [SPECIAL_TOKENS[\"</board>\"],\n",
        "             SPECIAL_TOKENS[\"<moves>\"]] +\n",
        "            future_tokens +\n",
        "            [SPECIAL_TOKENS[\"</moves>\"]]\n",
        "        )\n",
        "        moves_start = len(fen_tokens) + 2  # <board>, </board>\n",
        "        labels = (\n",
        "            [self.pad_token] * (moves_start + 1) +  # pad up through <moves>\n",
        "            future_tokens +\n",
        "            [self.pad_token]                     # do not predict </moves>\n",
        "        )\n",
        "\n",
        "        # 5) truncate & pad\n",
        "        input_seq = (input_seq + [self.pad_token] * self.max_seq_len)[:self.max_seq_len]\n",
        "        labels    = (labels    + [self.pad_token] * self.max_seq_len)[:self.max_seq_len]\n",
        "\n",
        "        return torch.tensor(input_seq, dtype=torch.long), \\\n",
        "               torch.tensor(labels,    dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy0TprDuIvWQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1) Fix your random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# 2) Shuffle indices and split\n",
        "n = len(GAMES)\n",
        "indices = list(range(n))\n",
        "random.shuffle(indices)\n",
        "split = int(n * 0.8)\n",
        "train_idx, test_idx = indices[:split], indices[split:]\n",
        "\n",
        "# 3) Slice out train / test game‐lists\n",
        "train_games = [GAMES[i] for i in train_idx]\n",
        "test_games  = [GAMES[i] for i in test_idx]\n",
        "\n",
        "# 4) Create datasets\n",
        "max_len = 196\n",
        "train_ds = ChessGameDataset(train_games, max_seq_len=max_len)\n",
        "test_ds  = ChessGameDataset(test_games,  max_seq_len=max_len)\n",
        "\n",
        "# 5) Create loaders\n",
        "bsz = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=bsz, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=bsz, shuffle=False)\n",
        "\n",
        "# Visualize\n",
        "for idx, (batch, labels) in enumerate(train_loader):\n",
        "    print(batch[0], labels[0])\n",
        "    print(\"Decoded batch:\")\n",
        "    print(untokenize(batch[0].tolist()))\n",
        "    print(\"Decoded labels:\")\n",
        "    print(untokenize(labels[0].tolist()))\n",
        "    if idx == 1:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uJzGDO_MBWX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def make_custom_attention_mask(input_ids: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs a (B, S, S) attention mask:\n",
        "    - Bidirectional within <board>...</board>\n",
        "    - Causal within <moves>...</moves>\n",
        "    - No attention for <pad>\n",
        "    \"\"\"\n",
        "    B, S = input_ids.shape\n",
        "    mask = torch.full((B, S, S), float('-inf'), device=input_ids.device)\n",
        "\n",
        "    for b in range(B):\n",
        "        row = input_ids[b]\n",
        "\n",
        "        board_start = (row == SPECIAL_TOKENS[\"<board>\"]).nonzero(as_tuple=False)\n",
        "        board_end = (row == SPECIAL_TOKENS[\"</board>\"]).nonzero(as_tuple=False)\n",
        "        moves_start = (row == SPECIAL_TOKENS[\"<moves>\"]).nonzero(as_tuple=False)\n",
        "        moves_end = (row == SPECIAL_TOKENS[\"</moves>\"]).nonzero(as_tuple=False)\n",
        "\n",
        "        # Bidirectional attention inside <board>...</board>\n",
        "        if len(board_start) and len(board_end):\n",
        "            s, e = board_start.item(), board_end.item()\n",
        "            mask[b, s:e+1, s:e+1] = 0\n",
        "\n",
        "        # Causal attention inside <moves>...</moves>\n",
        "        if len(moves_start):\n",
        "            s = moves_start.item()\n",
        "            e = moves_end.item() if len(moves_end) else S - 1\n",
        "            e = min(e, S - 1)  # clamp\n",
        "            for i in range(s, e + 1):\n",
        "                mask[b, i, s:i+1] = 0\n",
        "\n",
        "        # Block <pad> tokens\n",
        "        pad_mask = row == SPECIAL_TOKENS[\"<pad>\"]\n",
        "        mask[b, pad_mask, :] = float('-inf')\n",
        "        mask[b, :, pad_mask] = float('-inf')\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "class AutoregressiveTransformer(nn.Module):\n",
        "    \"\"\"A minimal decoder‑only (causal) Transformer with **learned** positional embeddings.\n",
        "\n",
        "    ‑ ``batch_first`` everywhere\n",
        "    ‑ uses ``nn.Embedding`` for positions instead of sinusoidal features\n",
        "    ‑ retains a simple causal mask for autoregressive modelling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_heads: int = 8,\n",
        "        num_layers: int = 6,\n",
        "        d_ff: int = 2048,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        pad_id: int = 0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.pad_id = pad_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Transformer blocks (encoder‑layer reused for decoder‑only stack)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # Language‑model head shares weight matrix with token_emb if desired (not tied here)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize position embedding with small variance\n",
        "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    # ‑‑‑ utility masks ‑‑‑ -------------------------------------------------\n",
        "    def _causal_mask(self, size: int, device: torch.device) -> torch.Tensor:\n",
        "        \"\"\"Upper‑triangular causal mask (True means *masked* for PyTorch).\"\"\"\n",
        "        return torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
        "\n",
        "    # ‑‑‑ forward pass ‑‑‑ ---------------------------------------------------\n",
        "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
        "      bsz, seq_len = tokens.shape\n",
        "      if seq_len > self.max_len:\n",
        "          raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}.\")\n",
        "\n",
        "      pos = torch.arange(seq_len, device=tokens.device).unsqueeze(0).expand(bsz, seq_len)\n",
        "      x = self.token_emb(tokens) + self.pos_emb(pos)\n",
        "\n",
        "      attn_mask = self._causal_mask(size=seq_len, device=tokens.device)# make_custom_attention_mask(tokens)  # (B, S, S)\n",
        "      # attn_mask = attn_mask.repeat_interleave(self.n_heads, dim=0)\n",
        "\n",
        "      pad_mask = tokens.eq(self.pad_id)               # (B, S)\n",
        "\n",
        "      x = self.transformer(x, mask=attn_mask, src_key_padding_mask=pad_mask)\n",
        "      return self.lm_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UghmukxXcK-g"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "vocab_size = max(SPECIAL_TOKENS.values()) + 1\n",
        "model = AutoregressiveTransformer(vocab_size=vocab_size, pad_id=SPECIAL_TOKENS['<pad>'], d_model=1_024, d_ff=4_096, num_layers=8, max_len=256+1).cuda()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "c = 0\n",
        "for pp in model.parameters():\n",
        "    c += pp.numel()\n",
        "print(\"Total parameters:\", c)\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKlH2WiBMBav"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import chess\n",
        "import chess.svg\n",
        "import chess.pgn\n",
        "import torch.nn.functional as F\n",
        "import cairosvg\n",
        "import imageio.v2 as imageio\n",
        "import os\n",
        "import random\n",
        "import base64\n",
        "from typing import Optional\n",
        "from tempfile import TemporaryDirectory\n",
        "from IPython.display import SVG, display, Video, HTML\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————————————\n",
        "# 1) FULL-FEN TOKENIZER\n",
        "#    consumes every character in the 6-field FEN string\n",
        "def tokenize_fen(fen: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Takes the full FEN, e.g.:\n",
        "      \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "    and returns a list of character-IDs for every character that\n",
        "    appears in our FEN_CHAR_TO_ID vocabulary.\n",
        "    \"\"\"\n",
        "    return [FEN_CHAR_TO_ID[c] for c in fen if c in FEN_CHAR_TO_ID]\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————————————\n",
        "# 2) SAMPLE + LOG-PROB DIAGNOSTIC\n",
        "@torch.no_grad()\n",
        "def sample_model_moves(model, dataset, max_moves=10, temperature=1.0, top_k=10):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # pick random\n",
        "    idx = random.randrange(len(dataset))\n",
        "    input_ids, labels = dataset[idx]\n",
        "    input_ids = input_ids.unsqueeze(0).to(device)\n",
        "    labels   = labels.unsqueeze(0).to(device)\n",
        "\n",
        "    # extract full-fen from input_ids\n",
        "    bs = input_ids[0]\n",
        "    b0 = (bs == SPECIAL_TOKENS[\"<board>\"]).nonzero(as_tuple=True)[0].item() + 1\n",
        "    b1 = (bs == SPECIAL_TOKENS[\"</board>\"]).nonzero(as_tuple=True)[0].item()\n",
        "    fen_ids = bs[b0:b1].tolist()\n",
        "    # rebuild full-fen string\n",
        "    fen_chars = [ID_TO_FEN_CHAR[i] for i in fen_ids]\n",
        "    fen_full = \"\".join(fen_chars)\n",
        "    # now fen_full contains e.g. \"rnbqkbnr/... w KQkq - 0 1\"\n",
        "    board = chess.Board(fen=fen_full)\n",
        "\n",
        "    display(SVG(chess.svg.board(board=board, size=350)))\n",
        "    print(\"📍 Full FEN:\", fen_full, \"\\n\")\n",
        "\n",
        "    # show tokens\n",
        "    print(\"Input tokens: \", \" \".join(untokenize(input_ids[0].tolist())))\n",
        "    print(\"Label tokens:\", \" \".join(untokenize(labels[0].tolist())), \"\\n\")\n",
        "\n",
        "    # teacher-forcing loss\n",
        "    inp  = input_ids[:, :-1]\n",
        "    targ = labels[:,  1:]\n",
        "    logits = model(inp)                   # (1, S-1, V)\n",
        "    logp   = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    ll = []\n",
        "    for i in range(targ.size(1)):\n",
        "        tid = targ[0, i].item()\n",
        "        if tid == SPECIAL_TOKENS[\"<pad>\"]:\n",
        "            continue\n",
        "        p = logp[0, i, tid].item()\n",
        "        print(f\" Step {i:2d} P({untokenize([tid])[0]}) = {math.exp(p):.4f}  logp={p:.4f}\")\n",
        "        ll.append(p)\n",
        "    if ll:\n",
        "        avg = sum(ll)/len(ll)\n",
        "        print(f\"\\n→ mean NLL: {-avg:.4f}, ppl: {math.exp(-avg):.2f}\\n\")\n",
        "    else:\n",
        "        print(\"⚠️ no valid labels for loss\\n\")\n",
        "\n",
        "    # sample future moves\n",
        "    ms = SPECIAL_TOKENS[\"<moves>\"]\n",
        "    me = SPECIAL_TOKENS[\"</moves>\"]\n",
        "    start = (bs==ms).nonzero(as_tuple=True)[0].item()+1\n",
        "    gen = bs[:start].tolist()\n",
        "\n",
        "    for _ in range(max_moves):\n",
        "        x = torch.tensor(gen, device=device).unsqueeze(0)\n",
        "        lg = model(x)[0,-1,:]/temperature\n",
        "        if top_k:\n",
        "            v,i = torch.topk(lg, top_k)\n",
        "            pr = F.softmax(v, dim=0)\n",
        "            nxt = i[torch.multinomial(pr,1)].item()\n",
        "        else:\n",
        "            pr = F.softmax(lg, dim=-1)\n",
        "            nxt = torch.multinomial(pr,1).item()\n",
        "        if nxt in (me, SPECIAL_TOKENS[\"<pad>\"]):\n",
        "            break\n",
        "        gen.append(nxt)\n",
        "\n",
        "    sampled = [UCI_IDS[t] for t in gen[start:] if t in UCI_IDS]\n",
        "    actual  = [UCI_IDS[t] for t in labels[0,start:].tolist() if t in UCI_IDS]\n",
        "    print(\"🔮 Sampled:\", \" \".join(sampled))\n",
        "    print(\"✅ Ground truth:\", \" \".join(actual))\n",
        "    return sampled\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————————————\n",
        "# 3) SAMPLE FULL GAME → MP4 AT CORRECT FEN\n",
        "@torch.no_grad()\n",
        "def sample_game_to_video(\n",
        "    model,\n",
        "    max_moves: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 10,\n",
        "    video_path: str = \"sample_game.mp4\",\n",
        "    frame_duration: float = 1.2\n",
        ") -> Optional[chess.pgn.Game]:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # start from initial\n",
        "    board = chess.Board()\n",
        "    # you can also push a first move, e.g. board.push_uci(\"e2e4\")\n",
        "\n",
        "    # build initial input\n",
        "    fen_ids = tokenize_fen(board.fen())\n",
        "    seq = ([SPECIAL_TOKENS[\"<board>\"]] +\n",
        "           fen_ids +\n",
        "           [SPECIAL_TOKENS[\"</board>\"], SPECIAL_TOKENS[\"<moves>\"]])\n",
        "    gen = torch.tensor(seq, device=device).unsqueeze(0)[0].tolist()\n",
        "\n",
        "    game = chess.pgn.Game()\n",
        "    last = None\n",
        "\n",
        "    fps = 1.0/frame_duration\n",
        "    with TemporaryDirectory() as tmp:\n",
        "        frames = []\n",
        "        # frame 0\n",
        "        svg0 = chess.svg.board(board=board, size=350, lastmove=last)\n",
        "        p0   = os.path.join(tmp, \"f000.svg\")\n",
        "        png0 = os.path.join(tmp, \"f000.png\")\n",
        "        open(p0,\"w\").write(svg0)\n",
        "        cairosvg.svg2png(url=p0, write_to=png0)\n",
        "        frames.append(png0)\n",
        "\n",
        "        for i in range(1, max_moves+1):\n",
        "            x = torch.tensor(gen, device=device).unsqueeze(0)\n",
        "            lg = model(x)[0,-1,:]/temperature\n",
        "            if top_k:\n",
        "                v,iid = torch.topk(lg, top_k)\n",
        "                pr     = F.softmax(v, dim=0)\n",
        "                tok    = iid[torch.multinomial(pr,1)].item()\n",
        "            else:\n",
        "                pr  = F.softmax(lg,dim=-1)\n",
        "                tok = torch.multinomial(pr,1).item()\n",
        "            if tok in (SPECIAL_TOKENS[\"</moves>\"], SPECIAL_TOKENS[\"<pad>\"]):\n",
        "                break\n",
        "            gen.append(tok)\n",
        "            if tok not in UCI_IDS:\n",
        "                print(\"⚠️ unk\", tok); break\n",
        "            m = chess.Move.from_uci(UCI_IDS[tok])\n",
        "            if not board.is_legal(m):\n",
        "                print(\"⛔ illegal\", UCI_IDS[tok]); break\n",
        "            board.push(m); last=m; game.add_variation(m)\n",
        "\n",
        "            svgi = chess.svg.board(board=board, size=350, lastmove=last)\n",
        "            pi   = os.path.join(tmp, f\"f{i:03}.svg\")\n",
        "            pngi = os.path.join(tmp, f\"f{i:03}.png\")\n",
        "            open(pi,\"w\").write(svgi)\n",
        "            cairosvg.svg2png(url=pi, write_to=pngi)\n",
        "            frames.append(pngi)\n",
        "\n",
        "        # write mp4 with correct fps\n",
        "        writer = imageio.get_writer(video_path, format=\"ffmpeg\", fps=fps)\n",
        "        for p in frames:\n",
        "            writer.append_data(imageio.imread(p))\n",
        "        writer.close()\n",
        "\n",
        "    display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))\n",
        "    print(f\"✅ Video saved @ {fps:.2f} fps → {video_path}\")\n",
        "    return game\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import chess\n",
        "import chess.pgn\n",
        "import chess.svg\n",
        "import torch.nn.functional as F\n",
        "import cairosvg\n",
        "import imageio.v2 as imageio\n",
        "import os\n",
        "import random\n",
        "import base64\n",
        "from typing import Optional\n",
        "from tempfile import TemporaryDirectory\n",
        "from IPython.display import Video, display\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_game_masked(\n",
        "    model,\n",
        "    max_moves: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    video_path: str = \"sample_game_masked.mp4\",\n",
        "    frame_duration: float = 1.2  # seconds per frame\n",
        ") -> Optional[chess.pgn.Game]:\n",
        "    \"\"\"\n",
        "    Samples a full game, but at each step:\n",
        "      • Enumerates board.legal_moves\n",
        "      • Converts them to your UCI token IDs\n",
        "      • Masks out all other logits\n",
        "      • Samples from the remaining legal‐move distribution\n",
        "    Saves as MP4 at fps = 1/frame_duration.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    board = chess.Board()\n",
        "    # Optionally seed first move:\n",
        "    # board.push_uci(\"e2e4\")\n",
        "\n",
        "    # Build initial context\n",
        "    fen_ids = tokenize_fen(board.fen())\n",
        "    seq = [SPECIAL_TOKENS[\"<board>\"]] + fen_ids + [\n",
        "        SPECIAL_TOKENS[\"</board>\"], SPECIAL_TOKENS[\"<moves>\"]\n",
        "    ]\n",
        "    generated = seq.copy()\n",
        "    game = chess.pgn.Game()\n",
        "    last_move = None\n",
        "\n",
        "    fps = 1.0 / frame_duration\n",
        "\n",
        "    with TemporaryDirectory() as tmpdir:\n",
        "        frames = []\n",
        "        # frame 0: starting position\n",
        "        svg0 = chess.svg.board(board=board, size=350, lastmove=last_move)\n",
        "        p0   = os.path.join(tmpdir, \"frame_000.svg\")\n",
        "        png0 = os.path.join(tmpdir, \"frame_000.png\")\n",
        "        open(p0, \"w\").write(svg0)\n",
        "        cairosvg.svg2png(url=p0, write_to=png0)\n",
        "        frames.append(png0)\n",
        "\n",
        "        for i in range(1, max_moves+1):\n",
        "            x = torch.tensor(generated, device=device).unsqueeze(0)\n",
        "            logits = model(x)[0, -1, :] / temperature  # (V,)\n",
        "\n",
        "            # build mask over vocab, -inf for illegal moves\n",
        "            legal_ids = []\n",
        "            for mv in board.legal_moves:\n",
        "                uci = mv.uci()\n",
        "                if uci in UCI_MOVES:\n",
        "                    legal_ids.append(UCI_MOVES[uci])\n",
        "            if not legal_ids:\n",
        "                print(\"⛔ No legal moves tokenized – stopping.\")\n",
        "                break\n",
        "\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            mask[legal_ids] = 0.0\n",
        "            filtered_logits = logits + mask\n",
        "\n",
        "            probs = F.softmax(filtered_logits, dim=-1)\n",
        "            token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            # stop if end-of-moves or pad\n",
        "            if token in (SPECIAL_TOKENS[\"</moves>\"], SPECIAL_TOKENS[\"<pad>\"]):\n",
        "                break\n",
        "            generated.append(token)\n",
        "\n",
        "            # apply move\n",
        "            uci = UCI_IDS.get(token, None)\n",
        "            if uci is None:\n",
        "                print(f\"⚠️ Generated unknown token {token}.\")\n",
        "                break\n",
        "            move = chess.Move.from_uci(uci)\n",
        "            if not board.is_legal(move):\n",
        "                print(f\"⛔ Illegal generated move: {uci}.\")\n",
        "                break\n",
        "\n",
        "            board.push(move)\n",
        "            last_move = move\n",
        "            game.add_variation(move)\n",
        "\n",
        "            # render frame\n",
        "            svg_i = chess.svg.board(board=board, size=350, lastmove=last_move)\n",
        "            pi   = os.path.join(tmpdir, f\"frame_{i:03}.svg\")\n",
        "            pngi = os.path.join(tmpdir, f\"frame_{i:03}.png\")\n",
        "            open(pi, \"w\").write(svg_i)\n",
        "            cairosvg.svg2png(url=pi, write_to=pngi)\n",
        "            frames.append(pngi)\n",
        "\n",
        "        # write MP4\n",
        "        with imageio.get_writer(video_path, format='ffmpeg', fps=fps) as writer:\n",
        "            for p in frames:\n",
        "                writer.append_data(imageio.imread(p))\n",
        "\n",
        "    display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))\n",
        "    print(f\"✅ Masked‐sampling video saved to {video_path} @ {fps:.2f} fps\")\n",
        "    return game\n"
      ],
      "metadata": {
        "id": "eK-XKVM9jUMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK2i_s1Ev0MY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/chess_checkpoints'\n",
        "name = 'v1'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "eatVsSrgi615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/chess_v1_vocab_size={vocab_size-1}_pad_id={SPECIAL_TOKENS['<pad>']}_d_model=1_024_d_ff=4_096_num_layers=8_latest.pt\"\n",
        "CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/chess_v1_vocab_size=2008_pad_id=2006_d_model=1_024_d_ff=4_096_num_layers=8_latest.pt\"\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.load_state_dict(ckpt['model_state'])\n",
        "optimizer.load_state_dict(ckpt['opt_state'])\n",
        "start_epoch = ckpt['epoch'] + 1\n",
        "last_loss   = ckpt['loss']\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model.to(device)         # e.g. device = torch.device('cuda')\n",
        "print(f\"✅ Loaded checkpoint from epoch {ckpt['epoch']}, loss={last_loss:.4f}\")"
      ],
      "metadata": {
        "id": "G0kabcP3Xpzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sample_game_to_video(model, max_moves=200, frame_duration=0.5, top_k=5)"
      ],
      "metadata": {
        "id": "jU_B2RmyzE2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sample_game_masked(model,\n",
        "                       max_moves=200,\n",
        "                       temperature=1.0,\n",
        "                       frame_duration=0.5,\n",
        "                       video_path=\"chess_masked.mp4\")"
      ],
      "metadata": {
        "id": "CoqpHlmdy_vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import chess\n",
        "import chess.svg\n",
        "import chess.pgn\n",
        "import cairosvg\n",
        "import imageio.v2 as imageio\n",
        "import os\n",
        "import random\n",
        "from tempfile import TemporaryDirectory\n",
        "from IPython.display import SVG, Video, display\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_game_alpha(\n",
        "    model,\n",
        "    alpha: Optional[int] = None,     # reset every alpha moves; None → never reset\n",
        "    max_moves: int = 50,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = 10,\n",
        "    video_path: str = \"sample_game_alpha.mp4\",\n",
        "    frame_duration: float = 1.2\n",
        ") -> chess.pgn.Game:\n",
        "    \"\"\"\n",
        "    Sample up to max_moves, resetting context every `alpha` moves.\n",
        "    If alpha is None or >= max_moves, context is never reset (full-history).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Initialize board + PGN\n",
        "    board = chess.Board()\n",
        "    game = chess.pgn.Game()\n",
        "    last_move = None\n",
        "\n",
        "    # Helper that builds the current context from the board\n",
        "    def make_ctx():\n",
        "        fen_ids = tokenize_fen(board.fen())\n",
        "        return [\n",
        "            SPECIAL_TOKENS[\"<board>\"],\n",
        "            *fen_ids,\n",
        "            SPECIAL_TOKENS[\"</board>\"],\n",
        "            SPECIAL_TOKENS[\"<moves>\"]\n",
        "        ]\n",
        "\n",
        "    generated = make_ctx()\n",
        "    moves_this_block = 0\n",
        "\n",
        "    fps = 1.0 / frame_duration\n",
        "    with TemporaryDirectory() as tmpdir:\n",
        "        frames = []\n",
        "\n",
        "        # --- Frame 0: starting position ---\n",
        "        svg0 = chess.svg.board(board=board, size=350, lastmove=last_move)\n",
        "        p0   = os.path.join(tmpdir, \"f000.svg\")\n",
        "        png0 = os.path.join(tmpdir, \"f000.png\")\n",
        "        open(p0, \"w\").write(svg0)\n",
        "        cairosvg.svg2png(url=p0, write_to=png0)\n",
        "        frames.append(png0)\n",
        "\n",
        "        for step in range(max_moves):\n",
        "            # reset context if we've reached the block size\n",
        "            if alpha is not None and moves_this_block >= alpha:\n",
        "                generated = make_ctx()\n",
        "                moves_this_block = 0\n",
        "\n",
        "            # model forward\n",
        "            x = torch.tensor(generated, device=device).unsqueeze(0)  # (1, L)\n",
        "            logits = model(x)[0, -1, :] / temperature              # (V,)\n",
        "\n",
        "            # gather only legal moves\n",
        "            legal_ids = [UCI_MOVES[m.uci()] for m in board.legal_moves if m.uci() in UCI_MOVES]\n",
        "            if not legal_ids:\n",
        "                print(\"⛔ no legal moves left – stopping.\")\n",
        "                break\n",
        "\n",
        "            # sample from legal logits directly\n",
        "            legal_logits = logits[legal_ids]                       # (L,)\n",
        "            probs        = F.softmax(legal_logits, dim=-1)         # (L,)\n",
        "\n",
        "            # optionally cap to top_k among the legal moves\n",
        "            if top_k is not None and top_k < probs.size(0):\n",
        "                vals, idxs = torch.topk(probs, top_k)\n",
        "                probs      = vals\n",
        "                legal_ids  = [legal_ids[i] for i in idxs.tolist()]\n",
        "\n",
        "            choice = torch.multinomial(probs, num_samples=1).item()\n",
        "            token  = legal_ids[choice]\n",
        "\n",
        "            # stop on special end‐of‐moves tokens\n",
        "            if token in (SPECIAL_TOKENS[\"</moves>\"], SPECIAL_TOKENS[\"<pad>\"]):\n",
        "                break\n",
        "\n",
        "            # append, apply to board, record PGN\n",
        "            generated.append(token)\n",
        "            uci = UCI_IDS[token]\n",
        "            move = chess.Move.from_uci(uci)\n",
        "            if not board.is_legal(move):\n",
        "                print(f\"⛔ illegal move {uci} – stopping.\")\n",
        "                break\n",
        "            board.push(move)\n",
        "            game.add_variation(move)\n",
        "            last_move = move\n",
        "            moves_this_block += 1\n",
        "\n",
        "            # render new frame\n",
        "            svg_i = chess.svg.board(board=board, size=350, lastmove=last_move)\n",
        "            pi    = os.path.join(tmpdir, f\"f{step+1:03}.svg\")\n",
        "            png_i = os.path.join(tmpdir, f\"f{step+1:03}.png\")\n",
        "            open(pi, \"w\").write(svg_i)\n",
        "            cairosvg.svg2png(url=pi, write_to=png_i)\n",
        "            frames.append(png_i)\n",
        "\n",
        "        # write out MP4\n",
        "        writer = imageio.get_writer(video_path, format='ffmpeg', fps=fps)\n",
        "        for p in frames:\n",
        "            writer.append_data(imageio.imread(p))\n",
        "        writer.close()\n",
        "\n",
        "    display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))\n",
        "    print(f\"✅ Video saved @ {fps:.2f} fps → {video_path}\")\n",
        "    return game\n"
      ],
      "metadata": {
        "id": "R09_Eywt7n56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# never reset (current behavior)\n",
        "sample_game_alpha(model, alpha=None, frame_duration=0.5)\n",
        "\n",
        "# reset every move\n",
        "sample_game_alpha(model, alpha=1, frame_duration=0.5)\n",
        "\n",
        "# reset every 4 moves\n",
        "sample_game_alpha(model, alpha=4, frame_duration=0.5)"
      ],
      "metadata": {
        "id": "tyGjojMu7qKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Adh8gl-ksAq"
      },
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch,100):\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        x = x.cuda(); y = y.cuda()\n",
        "        input_seq = x[:, :-1]\n",
        "        target_seq = y[:, 1:]\n",
        "\n",
        "        logits = model(input_seq)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq.reshape(-1), ignore_index=SPECIAL_TOKENS['<pad>'])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch} Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if step % 500 == 0:\n",
        "          print('No masking:')\n",
        "          _ = sample_game_to_video(model, max_moves=200, frame_duration=0.5, top_k=5)\n",
        "          print(\"Masking:\")\n",
        "          _ = sample_game_masked(model,\n",
        "                       max_moves=200,\n",
        "                       temperature=1.0,\n",
        "                       frame_duration=0.5,\n",
        "                       video_path=\"chess_masked.mp4\")\n",
        "\n",
        "\n",
        "    CHECKPOINT_PATH = f'{CHECKPOINT_DIR}/chess_{name}_vocab_size={vocab_size}_pad_id={SPECIAL_TOKENS[\"<pad>\"]}_d_model=1_024_d_ff=4_096_num_layers=8_latest.pt'\n",
        "    torch.save({\n",
        "        'epoch':      epoch,\n",
        "        'model_state': model.state_dict(),\n",
        "        'opt_state':  optimizer.state_dict(),\n",
        "        'loss':       loss,\n",
        "    }, CHECKPOINT_PATH)\n",
        "    print(f\"✅ Checkpoint saved to {CHECKPOINT_PATH}\")\n",
        "    if epoch % 5 == 0:\n",
        "      CHECKPOINT_PATH = f'{CHECKPOINT_DIR}/chess_{name}_vocab_size={vocab_size}_pad_id={SPECIAL_TOKENS[\"<pad>\"]}_d_model=1_024_d_ff=4_096_num_layers=8_epoch={epoch}.pt'\n",
        "      torch.save({\n",
        "          'epoch':      epoch,\n",
        "          'model_state': model.state_dict(),\n",
        "          'opt_state':  optimizer.state_dict(),\n",
        "          'loss':       loss,\n",
        "      }, CHECKPOINT_PATH)\n",
        "      print(f\"✅ Checkpoint saved to {CHECKPOINT_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVOl-qyZvuYA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3) Save a checkpoint\n",
        "# Assuming you have:\n",
        "#   model   -> your nn.Module\n",
        "#   optimizer -> your optimizer (e.g. AdamW)\n",
        "#   epoch   -> current epoch number (int)\n",
        "#   loss    -> last loss value (float)\n",
        "\n",
        "torch.save({\n",
        "    'epoch':      epoch,\n",
        "    'model_state': model.state_dict(),\n",
        "    'opt_state':  optimizer.state_dict(),\n",
        "    'loss':       loss,\n",
        "}, CHECKPOINT_PATH)\n",
        "print(f\"✅ Checkpoint saved to {CHECKPOINT_PATH}\")\n",
        "\n",
        "# 4) Later, to load it back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtI4voBppOs9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXJHnTnoqO70"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_TOKENS.update({\n",
        "    \"<think>\" : max(SPECIAL_TOKENS.values()) + 1,\n",
        "    \"</think>\": max(SPECIAL_TOKENS.values()) + 2,\n",
        "})\n",
        "ID_TO_SPECIAL.update({v: k for k, v in SPECIAL_TOKENS.items()})\n"
      ],
      "metadata": {
        "id": "jlwjmHXrM3ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerBase\n",
        "import json, os, re\n",
        "\n",
        "ALL_IDS = {\n",
        "    **UCI_MOVES,                      # 0 … |UCI|\n",
        "    **FEN_CHAR_TO_ID,                 # cont’d\n",
        "    **SPECIAL_TOKENS                  # cont’d\n",
        "}\n",
        "ID_TO_TOKEN = {v: k for k, v in ALL_IDS.items()}\n",
        "\n",
        "class ChessTokenizer(PreTrainedTokenizerBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            pad_token=\"<pad>\",\n",
        "            eos_token=\"</moves>\",      # generation finishes here\n",
        "        )\n",
        "        self.vocab = ALL_IDS\n",
        "        self.inv_vocab = ID_TO_TOKEN\n",
        "\n",
        "    # ----- required HF methods --------------------------------------------\n",
        "    def _tokenize(self, text):\n",
        "        # space-separated already – just split\n",
        "        return text.strip().split()\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        return self.vocab[token]\n",
        "\n",
        "    def _convert_id_to_token(self, idx):\n",
        "        return self.inv_vocab[idx]\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.vocab\n",
        "\n",
        "    # ----- helper ----------------------------------------------------------\n",
        "    def encode(self, s, **kw):\n",
        "        return [self.vocab[t] for t in self._tokenize(s)]\n",
        "\n",
        "    def decode(self, ids, **kw):\n",
        "        return \" \".join([self.inv_vocab[i] for i in ids\n",
        "                         if i != self.vocab[\"<pad>\"]])\n"
      ],
      "metadata": {
        "id": "NcyDnpUuM4ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(ID_TO_TOKEN.items())[-5:]"
      ],
      "metadata": {
        "id": "-4ZRTJnweBvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLThinkDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Returns only the prompt prefix; the trainer will call .generate() to\n",
        "    produce <think> … </think> <moves> MOVE </moves>.\n",
        "    \"\"\"\n",
        "    def __init__(self, games, max_len):\n",
        "        self.games, self.max_len = games, max_len\n",
        "        self.pad = SPECIAL_TOKENS[\"<pad>\"]\n",
        "\n",
        "    def __len__(self): return len(self.games)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        moves = [m.lower() for m in self.games[idx]]\n",
        "        # choose any *legal* random prefix so the position isn’t terminal\n",
        "        cutoff = random.randint(0, len(moves) - 1)\n",
        "        board  = chess.Board()\n",
        "        for m in moves[:cutoff]:\n",
        "            board.push_uci(m)\n",
        "        fen_tokens = tokenize_fen(board.fen())\n",
        "\n",
        "        prompt_ids = (\n",
        "            [SPECIAL_TOKENS[\"<board>\"]] +\n",
        "            fen_tokens +\n",
        "            [SPECIAL_TOKENS[\"</board>\"],\n",
        "             SPECIAL_TOKENS[\"<think>\"]]       # leave open tag\n",
        "        )\n",
        "\n",
        "        prompt_ids = prompt_ids[:self.max_len]\n",
        "        attn_mask  = [1]*len(prompt_ids)\n",
        "        pad_needed = self.max_len - len(prompt_ids)\n",
        "        prompt_ids += [self.pad]*pad_needed\n",
        "        attn_mask  += [0]*pad_needed\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(prompt_ids),\n",
        "            \"attention_mask\": torch.tensor(attn_mask),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "7kBDn4_nM6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ▸ run this in a Colab code cell\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y stockfish   # installs v16-series in <30 s"
      ],
      "metadata": {
        "id": "OvY71VLqNkpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIAL_TOKENS['<think>'] = max(SPECIAL_TOKENS.values()) + 1\n",
        "SPECIAL_TOKENS['</think>'] = max(SPECIAL_TOKENS.values()) + 2"
      ],
      "metadata": {
        "id": "dlaDr9-bQLDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, chess, chess.engine, torch\n",
        "os.environ[\"PATH\"] += \":/usr/games\"\n",
        "\n",
        "assert shutil.which(\"stockfish\")  # sanity check\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
        "board  = chess.Board()\n",
        "print(engine.analyse(board, chess.engine.Limit(depth=10)))\n",
        "engine.close()"
      ],
      "metadata": {
        "id": "Cr_AIjC2N2aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next()"
      ],
      "metadata": {
        "id": "h-P9fNw4eo5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chess.engine\n",
        "\n",
        "STOCKFISH_PATH = \"/usr/local/bin/stockfish\"   # adjust as needed\n",
        "engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
        "\n",
        "# regex to grab 1st UCI move between <moves> … </moves>\n",
        "MOVE_RE = re.compile(r\"<moves>\\s*([a-h][1-8][a-h][1-8][qrbn]?)\\s*</moves>\")\n",
        "tokenizer = ChessTokenizer()\n",
        "def reward_fn(batch_prompts, batch_outputs, limit=0.5, depth=12):\n",
        "    \"\"\"\n",
        "    vectorised reward: positive if predicted move improves Score(cp)\n",
        "    wrt the side to move, clipped to [-limit, +limit] and normalised.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for prompt_ids, generated_ids in zip(batch_prompts, batch_outputs):\n",
        "        prompt = tokenizer.decode(prompt_ids.tolist())\n",
        "        full   = tokenizer.decode(generated_ids.tolist())\n",
        "\n",
        "        # 1) recover FEN from prompt\n",
        "        fen_txt = prompt.split(\"</board>\")[0].split(\"<board>\")[1].strip()\n",
        "        board   = chess.Board(\" \".join(fen_txt))  # join chars back to string\n",
        "\n",
        "        # 2) extract move\n",
        "        m = MOVE_RE.search(full)\n",
        "        if m is None or not board.is_legal(chess.Move.from_uci(m[1])):\n",
        "            rewards.append(torch.tensor(-limit))   # illegal – punish\n",
        "            continue\n",
        "\n",
        "        move = chess.Move.from_uci(m[1])\n",
        "\n",
        "        # 3) Stockfish evaluations\n",
        "        with engine.analysis(board, chess.engine.Limit(depth=depth)) as info:\n",
        "            base_eval = info.info[\"score\"].pov(board.turn).white().score(mate_score=10000)\n",
        "        board.push(move)\n",
        "        with engine.analysis(board, chess.engine.Limit(depth=depth)) as info:\n",
        "            after_eval = info.info[\"score\"].pov(board.turn).white().score(mate_score=10000)\n",
        "\n",
        "        # positive if position improves for side to move\n",
        "        delta = (after_eval - base_eval) / 100.0          # centipawns → pawns\n",
        "        delta = max(min(delta,  limit), -limit)           # clip\n",
        "        rewards.append(torch.tensor(delta))\n",
        "    return torch.stack(rewards)\n",
        "engine.close()"
      ],
      "metadata": {
        "id": "Ke_6hhvtNLFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = RLThinkDataset(GAMES, max_len)"
      ],
      "metadata": {
        "id": "NgIC45WEPLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1VpaO1hd94G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, torch, chess, chess.svg, pprint, textwrap\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 0.  Verify FEN tokeniser round-trip ─────────────────────────────────────────\n",
        "fen_example = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "ids        = tokenize_fen(fen_example)\n",
        "fen_back   = \"\".join(ID_TO_FEN_CHAR[i] for i in ids)\n",
        "print(fen_example)\n",
        "print(fen_back)\n",
        "assert fen_back == fen_example, \"FEN round-trip failed!\"\n",
        "print(\"✅ Tokeniser round-trips FEN correctly.\\n\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1.  Helper: rebuild FEN *purely from IDs* ───────────────────────────────────\n",
        "def fen_from_prompt_ids(prompt_ids: torch.Tensor) -> str:\n",
        "    \"\"\"Read token IDs between <board> and </board> and convert to FEN string.\"\"\"\n",
        "    ids = prompt_ids.tolist()\n",
        "    try:\n",
        "        s = ids.index(SPECIAL_TOKENS[\"<board>\"]) + 1\n",
        "        e = ids.index(SPECIAL_TOKENS[\"</board>\"])\n",
        "    except ValueError as err:\n",
        "        raise ValueError(\"<board> or </board> missing\") from err\n",
        "    char_ids = ids[s:e]\n",
        "    fen = \"\".join(ID_TO_FEN_CHAR[i] for i in char_ids)\n",
        "    return fen\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2.  Visual sanity check on one dataset sample ───────────────────────────────\n",
        "def visual_debug(idx: int | None = None):\n",
        "    if idx is None:\n",
        "        idx = random.randrange(len(train_ds))\n",
        "\n",
        "    sample      = d[idx]\n",
        "    prompt_ids  = sample[\"input_ids\"]\n",
        "    prompt_txt  = tokenizer.decode(prompt_ids.tolist(), skip_special_tokens=False)\n",
        "    print(prompt_txt)\n",
        "    fen_str     = fen_from_prompt_ids(prompt_ids)\n",
        "    board       = chess.Board(fen_str)\n",
        "\n",
        "    # fabricate a dummy CoT & legal move so reward_fn has something to chew on\n",
        "    legal_move  = random.choice(list(board.legal_moves)).uci()\n",
        "    cot_txt     = \" e1e8\"\n",
        "    generated   = f\"{prompt_txt}{cot_txt} </think> <moves> {legal_move} </moves>\"\n",
        "    print((generated))\n",
        "    output_ids  = torch.tensor(tokenizer.encode(generated))\n",
        "\n",
        "    reward_val  = reward_fn(prompt_ids.unsqueeze(0),\n",
        "                            output_ids.unsqueeze(0)).item()\n",
        "\n",
        "    # ── display ──────────────────────────────────────────────────────────────\n",
        "    print(f\"\\nDataset index {idx}\")\n",
        "    print(\"Prompt tokens (decoded):\")\n",
        "    print(textwrap.fill(prompt_txt, width=90), \"\\n\")\n",
        "    print(\"Generated dummy output:\")\n",
        "    print(textwrap.fill(generated, width=90), \"\\n\")\n",
        "    print(f\"Reward = {reward_val:+.3f}\\n\")\n",
        "    display(SVG(chess.svg.board(board=board)))\n",
        "\n",
        "visual_debug()                # run; pass idx=42 to inspect a specific sample\n"
      ],
      "metadata": {
        "id": "YxsaB8i5PJ4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxtF2rq6qR-a"
      },
      "outputs": [],
      "source": [
        "# --- 1.  make a tiny HF config ---------------------------------------------\n",
        "from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "\n",
        "class ChessConfig(PretrainedConfig):\n",
        "    model_type = \"chess_transformer\"\n",
        "    def __init__(self, vocab_size: int, pad_token_id: int = 0, **kwargs):\n",
        "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
        "        self.vocab_size = vocab_size                    # needed by generate()\n",
        "\n",
        "# --- 2.  hug-friendly wrapper around your pure-torch model ------------------\n",
        "class HFChessModel(PreTrainedModel, GenerationMixin):\n",
        "    config_class = ChessConfig\n",
        "\n",
        "    def __init__(self, inner: AutoregressiveTransformer):\n",
        "        cfg = ChessConfig(vocab_size=inner.token_emb.num_embeddings,\n",
        "                          pad_token_id=inner.pad_id)\n",
        "        super().__init__(cfg)\n",
        "        self.inner = inner                             # keep real net inside\n",
        "\n",
        "    # ---- the two methods GRPOTrainer cares about ---------------------------\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        logits = self.inner(input_ids)                 # (B,S,V)\n",
        "        return CausalLMOutput(logits=logits)\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
        "        return {\"input_ids\": input_ids}\n",
        "\n",
        "# --- 3.  build policy / reference and launch trainer ------------------------\n",
        "policy      = HFChessModel(model).cuda()\n",
        "ref_policy  = HFChessModel(model).cuda()\n",
        "ref_policy.load_state_dict(policy.state_dict())        # frozen copy\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "args = GRPOConfig(output_dir=\"ckpts/chess-grpo\", logging_steps=10)\n",
        "'''\n",
        "trainer = GRPOTrainer(\n",
        "    model=policy,\n",
        "    ref_model=ref_policy,\n",
        "    args=args,\n",
        "    tokenizer=my_tokenizer,       # can be a dummy PreTrainedTokenizerFast\n",
        "    train_dataset=prompts_ds,     # any Dataset yielding {\"prompt\": str}\n",
        "    reward_funcs=my_reward_fn     # or list of funcs\n",
        ")\n",
        "trainer.train()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "id": "K4F_x-59LiPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnw-7YiNLlQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}