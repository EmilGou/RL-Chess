{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First two lines should be commented out in very first run, uncomment after\n",
        "# %cd ..\n",
        "# !rm -rf /content/RL-Chess-Transformers/\n",
        "!git clone https://github.com/EmilGou/RL-Chess-Transformers.git\n",
        "!rm -rf /content/RL-Chess-Transformers/GRPO/colab_notebooks\n",
        "\n",
        "%cd /content/RL-Chess-Transformers/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XquWMyVEZ0uX",
        "outputId": "7290fb0f-fbd9-456d-872d-73eb52973730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'RL-Chess-Transformers'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 157 (delta 96), reused 104 (delta 44), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (157/157), 3.09 MiB | 27.54 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "/content/RL-Chess-Transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "nhpnElIsjiVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afac0de-2967-4d5d-804b-923de5526df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoFS2SupnQt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b579a65-249d-40ae-9490-93c7ad6eb87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-chess in /usr/local/lib/python3.11/dist-packages (1.999)\n",
            "Requirement already satisfied: cairosvg in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: chess<2,>=1 in /usr/local/lib/python3.11/dist-packages (from python-chess) (1.11.2)\n",
            "Requirement already satisfied: cairocffi in /usr/local/lib/python3.11/dist-packages (from cairosvg) (1.7.1)\n",
            "Requirement already satisfied: cssselect2 in /usr/local/lib/python3.11/dist-packages (from cairosvg) (0.8.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cairosvg) (11.2.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from cairosvg) (1.4.0)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from cairocffi->cairosvg) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-chess cairosvg\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import chess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download raw moves file\n",
        "!pip install -U -q gdown\n",
        "FILE_ID=\"1BSBuF2dKOnVWuR5CNjp-o7QBYb-10JTO\"\n",
        "!gdown --id $FILE_ID -O moves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbBtEF9kRkol",
        "outputId": "325cea00-2d73-4843-dbda-5495ef174ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BSBuF2dKOnVWuR5CNjp-o7QBYb-10JTO\n",
            "From (redirected): https://drive.google.com/uc?id=1BSBuF2dKOnVWuR5CNjp-o7QBYb-10JTO&confirm=t&uuid=64ac678e-5e60-4bcf-ad5d-9e5d91bfec7d\n",
            "To: /content/RL-Chess-Transformers/moves\n",
            "100% 134M/134M [00:00<00:00, 205MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained Model\n",
        "# Dataset\n",
        "# Generation Function\n",
        "# GRPO Objective Function\n",
        "  # Reward Function\n",
        "    # Stockfish\n",
        "# Training Loop\n",
        "# Logging"
      ],
      "metadata": {
        "id": "Yruh3k-Wn6_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moves = open(\"moves\", \"r\").read()\n",
        "moves = moves.split('\\n\\n')[:-1]\n",
        "GAMES = [m.split('\\n')[:-1] for m in moves]"
      ],
      "metadata": {
        "id": "Ou0NJpDqoy6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GRPO.model import AutoregressiveTransformer, ChessConfig\n",
        "\n",
        "model = AutoregressiveTransformer(ChessConfig())\n",
        "model\n"
      ],
      "metadata": {
        "id": "9tFyj_ocpn2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170a768c-abcc-4554-eca5-f46fcd8cb12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoregressiveTransformer(\n",
              "  (token_emb): Embedding(2008, 512, padding_idx=2006)\n",
              "  (pos_emb): Embedding(512, 512)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=2008, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vydPwpTz8NAu",
        "outputId": "b6753db0-8e96-4884-ed96-721ea3094538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoregressiveTransformer(\n",
              "  (token_emb): Embedding(2008, 512, padding_idx=2006)\n",
              "  (pos_emb): Embedding(512, 512)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=2008, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GRPO.data import RLThinkDataset\n",
        "\n",
        "d = RLThinkDataset(GAMES)"
      ],
      "metadata": {
        "id": "6SJMN1tQ89YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!   wget https://github.com/official-stockfish/Stockfish/releases/latest/download/stockfish-ubuntu-x86-64-sse41-popcnt.tar && \\\n",
        "    tar -xf stockfish-ubuntu-x86-64-sse41-popcnt.tar stockfish/stockfish-ubuntu-x86-64-sse41-popcnt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI7mg0RDjopJ",
        "outputId": "1db963b1-cd28-441c-c03d-ff078b2e7fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-28 17:19:00--  https://github.com/official-stockfish/Stockfish/releases/latest/download/stockfish-ubuntu-x86-64-sse41-popcnt.tar\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/official-stockfish/Stockfish/releases/download/sf_17.1/stockfish-ubuntu-x86-64-sse41-popcnt.tar [following]\n",
            "--2025-05-28 17:19:00--  https://github.com/official-stockfish/Stockfish/releases/download/sf_17.1/stockfish-ubuntu-x86-64-sse41-popcnt.tar\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/20976138/1f1e24b8-d3b0-49bf-92fd-ebd01964592a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250528%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250528T171900Z&X-Amz-Expires=300&X-Amz-Signature=2aa773afa7637823415acb9c28f095a09c0d6f340c0a89a78847bb94ec0cfafb&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dstockfish-ubuntu-x86-64-sse41-popcnt.tar&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-05-28 17:19:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/20976138/1f1e24b8-d3b0-49bf-92fd-ebd01964592a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250528%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250528T171900Z&X-Amz-Expires=300&X-Amz-Signature=2aa773afa7637823415acb9c28f095a09c0d6f340c0a89a78847bb94ec0cfafb&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dstockfish-ubuntu-x86-64-sse41-popcnt.tar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80087040 (76M) [application/octet-stream]\n",
            "Saving to: ‘stockfish-ubuntu-x86-64-sse41-popcnt.tar’\n",
            "\n",
            "stockfish-ubuntu-x8 100%[===================>]  76.38M   211MB/s    in 0.4s    \n",
            "\n",
            "2025-05-28 17:19:01 (211 MB/s) - ‘stockfish-ubuntu-x86-64-sse41-popcnt.tar’ saved [80087040/80087040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chess, chess.engine, torch, numpy as np\n",
        "from GRPO.utils import extract_fen_from_game\n",
        "from GRPO.vocab import UCI_IDS, UCI_MOVES      # id → uci   and   uci → id\n",
        "\n",
        "path = \"stockfish/stockfish-ubuntu-x86-64-sse41-popcnt\"\n",
        "\n",
        "def _pad(seqs, pad_id):\n",
        "    max_len = max(len(s) for s in seqs)\n",
        "    return torch.tensor(\n",
        "        [s + [pad_id] * (max_len - len(s)) for s in seqs],\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "\n",
        "def _generate_completions_and_score(\n",
        "    model,\n",
        "    prompt_ids,                  # (B, T)\n",
        "    engine_path     = path,\n",
        "    depth           = 12,        # for Stockfish analyse\n",
        "    num_generations = 4,\n",
        "    num_moves       = 10,\n",
        "    limit = 5\n",
        "):\n",
        "    device, pad_id = prompt_ids.device, model.pad_id\n",
        "\n",
        "    # 1) repeat prompts: (B, T) → (B·G, T)\n",
        "    B, T       = prompt_ids.shape\n",
        "    input_ids  = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
        "    batch_size = input_ids.size(0)                 # B·G\n",
        "    seqs       = input_ids.tolist()\n",
        "\n",
        "    # 2) boards\n",
        "    boards = [chess.Board(extract_fen_from_game(s)) for s in seqs]\n",
        "\n",
        "    # 3) Stockfish eval before roll‑outs\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(engine_path)\n",
        "    with engine:\n",
        "        base_eval = torch.tensor(\n",
        "            [\n",
        "                engine.analyse(b, chess.engine.Limit(depth=depth))[\"score\"]\n",
        "                      .pov(b.turn).score(mate_score=10000)\n",
        "                for b in boards\n",
        "            ],\n",
        "            dtype=torch.float32, device=device\n",
        "        )\n",
        "\n",
        "        # 4) roll‑out:  num_moves × 2 half‑moves\n",
        "        completions, completion_masks = [[] for _ in range(batch_size)], [[] for _ in range(batch_size)]\n",
        "\n",
        "        for _ in range(num_moves):\n",
        "            # 4‑a) *policy* move  (mask = 1)\n",
        "            move_ids, input_ids = model.predict_move(\n",
        "                seqs,\n",
        "                _pad(seqs, pad_id).to(device),\n",
        "                boards,\n",
        "            )\n",
        "            for i, tok in enumerate(move_ids):\n",
        "                completions[i].append(tok)\n",
        "                completion_masks[i].append(1 if tok != pad_id else 0)\n",
        "                if tok != pad_id and not boards[i].is_game_over():\n",
        "                    boards[i].push_uci(UCI_IDS[tok])\n",
        "                seqs[i].append(tok)\n",
        "\n",
        "            # 4‑b) *model* response move  (mask = 0)\n",
        "            resp_ids, input_ids = model.predict_move(\n",
        "                seqs,\n",
        "                _pad(seqs, pad_id).to(device),\n",
        "                boards,\n",
        "            )\n",
        "            for i, tok in enumerate(resp_ids):\n",
        "                completions[i].append(tok)\n",
        "                completion_masks[i].append(0)           # never optimised\n",
        "                if tok != pad_id and not boards[i].is_game_over():\n",
        "                    boards[i].push_uci(UCI_IDS[tok])\n",
        "                seqs[i].append(tok)\n",
        "\n",
        "        # 5) Stockfish eval after roll‑outs\n",
        "        after_eval = torch.tensor(\n",
        "            [\n",
        "                engine.analyse(b, chess.engine.Limit(depth=depth))[\"score\"]\n",
        "                      .pov(b.turn).score(mate_score=10000)\n",
        "                for b in boards\n",
        "            ],\n",
        "            dtype=torch.float32, device=device\n",
        "        )\n",
        "\n",
        "    # 6) centred rewards → advantages\n",
        "    delta        = (after_eval - base_eval) / 100.0\n",
        "    scaled_reward = torch.clamp(delta, -limit, limit)\n",
        "    # ------------------------------------------------------------------\n",
        "#  ⬇  NEW: per‑token advantages (B·G, L)  --------------------------\n",
        "# ------------------------------------------------------------------\n",
        "    rewards   = scaled_reward.view(B, num_generations)          # (B, G) as before\n",
        "    centered  = rewards - rewards.mean(dim=1, keepdim=True)     # baseline\n",
        "\n",
        "    # Expand to (B·G, L) by repeating each reward over its two half‑moves\n",
        "    # (first is policy, second is response) …\n",
        "    expanded = centered.repeat_interleave(num_moves * 2, dim=1)   # (B, G*L)\n",
        "    expanded = expanded.view(-1, num_moves * 2)                    # (B·G, L)\n",
        "\n",
        "    # Cumulative future return per token (process supervision)\n",
        "    advantages = torch.flip(torch.cumsum(torch.flip(expanded, [1]), dim=1), [1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 7) pad everything for the backward pass\n",
        "    input_ids  = _pad(seqs, pad_id).to(device)                          # (B·G, T+L)\n",
        "    max_L      = max(len(c) for c in completions)\n",
        "    completion_ids  = torch.full((batch_size, max_L), pad_id,\n",
        "                                 dtype=torch.long, device=device)\n",
        "    completion_mask = torch.zeros_like(completion_ids, dtype=torch.float32)\n",
        "        # Zero‑out tokens that are masked out (responses & padding)\n",
        "\n",
        "    for i, (c, m) in enumerate(zip(completions, completion_masks)):\n",
        "        L = len(c)\n",
        "        completion_ids[i, :L]  = torch.tensor(c, dtype=torch.long, device=device)\n",
        "        completion_mask[i, :L] = torch.tensor(m, dtype=torch.float32, device=device)\n",
        "\n",
        "    advantages = advantages * completion_mask                     # (B·G, L)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"input_ids\"       : input_ids,        # (B·G, T+L)\n",
        "        \"completion_ids\"  : completion_ids,   # (B·G, L)\n",
        "        \"completion_mask\" : completion_mask,  # (B·G, L)\n",
        "        \"advantages\"      : advantages,       # (B·G,)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "64M6jDSE-Lyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(d, batch_size=8, shuffle=True)\n",
        "\n",
        "test_batch = next(iter(test_loader))\n",
        "test_batch.shape"
      ],
      "metadata": {
        "id": "LkYto5S_3HUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f6c29a-759e-435b-e260-e084e3b333a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn((5,5))\n",
        "b = torch.randn((5, 2))\n",
        "\n",
        "torch.cat([a,b], dim = 1).shape"
      ],
      "metadata": {
        "id": "7dhqF0hioMph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da3a0524-d4e4-4bbe-8756-aad7ae1cb016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GRPO.grpo_trainer import GRPOArgs, GRPOTrainer\n",
        "ref_model = AutoregressiveTransformer(ChessConfig()).to(device)\n",
        "trainer = GRPOTrainer(model, ref_model, GRPOArgs())"
      ],
      "metadata": {
        "id": "P_m70ZuB4ETW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = trainer._generate_completions_and_score(model, test_batch.to(device), path)\n",
        "loss = trainer._compute_loss(model, outputs)"
      ],
      "metadata": {
        "id": "w40mBUNIFPQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[\"advantages\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU5AUqQi4Qlf",
        "outputId": "1659c267-7ccb-4498-d4a3-3382d310f505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 40.0000,   0.0000,  39.8892,   0.0000,  39.2876,   0.0000,  38.0905,\n",
              "           0.0000,  36.1762,   0.0000,  33.4037,   0.0000,  29.6099,   0.0000,\n",
              "          24.6066,   0.0000,  18.1766,   0.0000,  10.0701,   0.0000],\n",
              "        [-40.0000,  -0.0000, -39.8892,  -0.0000, -39.2876,  -0.0000, -38.0905,\n",
              "          -0.0000, -36.1762,  -0.0000, -33.4037,  -0.0000, -29.6099,  -0.0000,\n",
              "         -24.6066,  -0.0000, -18.1766,  -0.0000, -10.0701,  -0.0000],\n",
              "        [ 40.0000,   0.0000,  39.8892,   0.0000,  39.2876,   0.0000,  38.0905,\n",
              "           0.0000,  36.1762,   0.0000,  33.4037,   0.0000,  29.6099,   0.0000,\n",
              "          24.6066,   0.0000,  18.1766,   0.0000,  10.0701,   0.0000],\n",
              "        [-40.0000,  -0.0000, -39.8892,  -0.0000, -39.2876,  -0.0000, -38.0905,\n",
              "          -0.0000, -36.1762,  -0.0000, -33.4037,  -0.0000, -29.6099,  -0.0000,\n",
              "         -24.6066,  -0.0000, -18.1766,  -0.0000, -10.0701,  -0.0000],\n",
              "        [ 19.3000,   0.0000,  19.2465,   0.0000,  18.9563,   0.0000,  18.3787,\n",
              "           0.0000,  17.4550,   0.0000,  16.1173,   0.0000,  14.2868,   0.0000,\n",
              "          11.8727,   0.0000,   8.7702,   0.0000,   4.8588,   0.0000],\n",
              "        [-57.9000,  -0.0000, -57.7396,  -0.0000, -56.8688,  -0.0000, -55.1360,\n",
              "          -0.0000, -52.3650,  -0.0000, -48.3518,  -0.0000, -42.8603,  -0.0000,\n",
              "         -35.6180,  -0.0000, -26.3106,  -0.0000, -14.5765,  -0.0000],\n",
              "        [ 19.3000,   0.0000,  19.2465,   0.0000,  18.9563,   0.0000,  18.3787,\n",
              "           0.0000,  17.4550,   0.0000,  16.1173,   0.0000,  14.2868,   0.0000,\n",
              "          11.8727,   0.0000,   8.7702,   0.0000,   4.8588,   0.0000],\n",
              "        [ 19.3000,   0.0000,  19.2465,   0.0000,  18.9563,   0.0000,  18.3787,\n",
              "           0.0000,  17.4550,   0.0000,  16.1173,   0.0000,  14.2868,   0.0000,\n",
              "          11.8727,   0.0000,   8.7702,   0.0000,   4.8588,   0.0000],\n",
              "        [ 20.0000,   0.0000,  19.9446,   0.0000,  19.6438,   0.0000,  19.0452,\n",
              "           0.0000,  18.0881,   0.0000,  16.7018,   0.0000,  14.8049,   0.0000,\n",
              "          12.3033,   0.0000,   9.0883,   0.0000,   5.0351,   0.0000],\n",
              "        [ 20.0000,   0.0000,  19.9446,   0.0000,  19.6438,   0.0000,  19.0452,\n",
              "           0.0000,  18.0881,   0.0000,  16.7018,   0.0000,  14.8049,   0.0000,\n",
              "          12.3033,   0.0000,   9.0883,   0.0000,   5.0351,   0.0000],\n",
              "        [ 20.0000,   0.0000,  19.9446,   0.0000,  19.6438,   0.0000,  19.0452,\n",
              "           0.0000,  18.0881,   0.0000,  16.7018,   0.0000,  14.8049,   0.0000,\n",
              "          12.3033,   0.0000,   9.0883,   0.0000,   5.0351,   0.0000],\n",
              "        [-60.0000,  -0.0000, -59.8338,  -0.0000, -58.9314,  -0.0000, -57.1357,\n",
              "          -0.0000, -54.2642,  -0.0000, -50.1055,  -0.0000, -44.4148,  -0.0000,\n",
              "         -36.9098,  -0.0000, -27.2649,  -0.0000, -15.1052,  -0.0000],\n",
              "        [ 20.3000,   0.0000,  20.2438,   0.0000,  19.9385,   0.0000,  19.3309,\n",
              "           0.0000,  18.3594,   0.0000,  16.9524,   0.0000,  15.0270,   0.0000,\n",
              "          12.4878,   0.0000,   9.2246,   0.0000,   5.1106,   0.0000],\n",
              "        [ 19.5000,   0.0000,  19.4460,   0.0000,  19.1527,   0.0000,  18.5691,\n",
              "           0.0000,  17.6359,   0.0000,  16.2843,   0.0000,  14.4348,   0.0000,\n",
              "          11.9957,   0.0000,   8.8611,   0.0000,   4.9092,   0.0000],\n",
              "        [-19.9000,  -0.0000, -19.8449,  -0.0000, -19.5456,  -0.0000, -18.9500,\n",
              "          -0.0000, -17.9976,  -0.0000, -16.6183,  -0.0000, -14.7309,  -0.0000,\n",
              "         -12.2418,  -0.0000,  -9.0429,  -0.0000,  -5.0099,  -0.0000],\n",
              "        [-19.9000,  -0.0000, -19.8449,  -0.0000, -19.5456,  -0.0000, -18.9500,\n",
              "          -0.0000, -17.9976,  -0.0000, -16.6183,  -0.0000, -14.7309,  -0.0000,\n",
              "         -12.2418,  -0.0000,  -9.0429,  -0.0000,  -5.0099,  -0.0000],\n",
              "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
              "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
              "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
              "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
              "        [-20.0000,  -0.0000, -19.9446,  -0.0000, -19.6438,  -0.0000, -19.0452,\n",
              "          -0.0000, -18.0881,  -0.0000, -16.7018,  -0.0000, -14.8049,  -0.0000,\n",
              "         -12.3033,  -0.0000,  -9.0883,  -0.0000,  -5.0351,  -0.0000],\n",
              "        [-20.0000,  -0.0000, -19.9446,  -0.0000, -19.6438,  -0.0000, -19.0452,\n",
              "          -0.0000, -18.0881,  -0.0000, -16.7018,  -0.0000, -14.8049,  -0.0000,\n",
              "         -12.3033,  -0.0000,  -9.0883,  -0.0000,  -5.0351,  -0.0000],\n",
              "        [ 60.0000,   0.0000,  59.8338,   0.0000,  58.9314,   0.0000,  57.1357,\n",
              "           0.0000,  54.2642,   0.0000,  50.1055,   0.0000,  44.4148,   0.0000,\n",
              "          36.9098,   0.0000,  27.2649,   0.0000,  15.1052,   0.0000],\n",
              "        [-20.0000,  -0.0000, -19.9446,  -0.0000, -19.6438,  -0.0000, -19.0452,\n",
              "          -0.0000, -18.0881,  -0.0000, -16.7018,  -0.0000, -14.8049,  -0.0000,\n",
              "         -12.3033,  -0.0000,  -9.0883,  -0.0000,  -5.0351,  -0.0000],\n",
              "        [  8.0500,   0.0000,   8.0277,   0.0000,   7.9066,   0.0000,   7.6657,\n",
              "           0.0000,   7.2805,   0.0000,   6.7225,   0.0000,   5.9590,   0.0000,\n",
              "           4.9521,   0.0000,   3.6580,   0.0000,   2.0266,   0.0000],\n",
              "        [-24.1500,  -0.0000, -24.0831,  -0.0000, -23.7199,  -0.0000, -22.9971,\n",
              "          -0.0000, -21.8414,  -0.0000, -20.1675,  -0.0000, -17.8770,  -0.0000,\n",
              "         -14.8562,  -0.0000, -10.9741,  -0.0000,  -6.0798,  -0.0000],\n",
              "        [  8.0500,   0.0000,   8.0277,   0.0000,   7.9066,   0.0000,   7.6657,\n",
              "           0.0000,   7.2805,   0.0000,   6.7225,   0.0000,   5.9590,   0.0000,\n",
              "           4.9521,   0.0000,   3.6580,   0.0000,   2.0266,   0.0000],\n",
              "        [  8.0500,   0.0000,   8.0277,   0.0000,   7.9066,   0.0000,   7.6657,\n",
              "           0.0000,   7.2805,   0.0000,   6.7225,   0.0000,   5.9590,   0.0000,\n",
              "           4.9521,   0.0000,   3.6580,   0.0000,   2.0266,   0.0000],\n",
              "        [  4.4500,   0.0000,   4.4377,   0.0000,   4.3707,   0.0000,   4.2376,\n",
              "           0.0000,   4.0246,   0.0000,   3.7162,   0.0000,   3.2941,   0.0000,\n",
              "           2.7375,   0.0000,   2.0221,   0.0000,   1.1203,   0.0000],\n",
              "        [  4.4500,   0.0000,   4.4377,   0.0000,   4.3707,   0.0000,   4.2376,\n",
              "           0.0000,   4.0246,   0.0000,   3.7162,   0.0000,   3.2941,   0.0000,\n",
              "           2.7375,   0.0000,   2.0221,   0.0000,   1.1203,   0.0000],\n",
              "        [  4.4500,   0.0000,   4.4377,   0.0000,   4.3707,   0.0000,   4.2376,\n",
              "           0.0000,   4.0246,   0.0000,   3.7162,   0.0000,   3.2941,   0.0000,\n",
              "           2.7375,   0.0000,   2.0221,   0.0000,   1.1203,   0.0000],\n",
              "        [-13.3500,  -0.0000, -13.3130,  -0.0000, -13.1122,  -0.0000, -12.7127,\n",
              "          -0.0000, -12.0738,  -0.0000, -11.1485,  -0.0000,  -9.8823,  -0.0000,\n",
              "          -8.2124,  -0.0000,  -6.0664,  -0.0000,  -3.3609,  -0.0000]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, chess, random\n",
        "# from pprint import pprint\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# # 1.  A minimal stub policy\n",
        "# # ---------------------------------------------------------------------\n",
        "# PAD_ID = 0          # choose any value that *isn't* a legal‑move id\n",
        "\n",
        "# try:\n",
        "#     from GRPO.vocab import UCI_MOVES, UCI_IDS\n",
        "# except ImportError:\n",
        "#     # quick‑n‑dirty fall‑backs for testing outside your repo\n",
        "#     UCI_MOVES, UCI_IDS = {}, {}\n",
        "\n",
        "# def _register(move):\n",
        "#     \"\"\"Add move to the two lookup dicts if unseen; return its id.\"\"\"\n",
        "#     if move not in UCI_MOVES:\n",
        "#         new_id = max(UCI_MOVES.values(), default=PAD_ID) + 1\n",
        "#         UCI_MOVES[move] = new_id\n",
        "#         UCI_IDS[new_id] = move\n",
        "#     return UCI_MOVES[move]\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# # 2.  Tiny prompts: one token per prompt is fine for a smoke test\n",
        "# # ---------------------------------------------------------------------\n",
        "# BATCH = test_batch.shape[0]\n",
        "# PROMPT_LEN = 1\n",
        "# prompt_ids = test_batch.to(device)\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# # 3.  Run the rollout\n",
        "# # ---------------------------------------------------------------------\n",
        "# # IMPORTANT: make sure the function we’re testing is already imported.\n",
        "\n",
        "# result = _generate_completions_and_score(\n",
        "#     model            = model,\n",
        "#     prompt_ids       = prompt_ids,\n",
        "#     num_generations  = 2,      # keep tiny for speed\n",
        "#     num_moves        = 3,      # six half‑moves per game\n",
        "#     depth            = 4,      # shallow Stockfish eval → fast\n",
        "# )\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# # 4.  Pretty‑print the outputs\n",
        "# # ---------------------------------------------------------------------\n",
        "# print(\"\\n=== Keys returned ===\")\n",
        "# print(list(result.keys()))\n",
        "\n",
        "# for k, v in result.items():\n",
        "#     print(f\"\\n--- {k} ---\")\n",
        "#     print(f\"shape: {tuple(v.shape)}  dtype: {v.dtype}\")\n",
        "#     # show first sample only, otherwise it’s a wall of numbers\n",
        "#     print(\"first row:\", v[0].tolist() if v.ndim > 1 else v.tolist())\n",
        "\n",
        "# # ---------------------------------------------------------------------\n",
        "# # 5.  Sanity assertions\n",
        "# # ---------------------------------------------------------------------\n",
        "# input_ids       = result[\"input_ids\"]\n",
        "# completion_ids  = result[\"completion_ids\"]\n",
        "# completion_mask = result[\"completion_mask\"]\n",
        "# advantages      = result[\"advantages\"]\n",
        "\n",
        "# B, G = BATCH, 2\n",
        "# assert input_ids.size(0) == completion_ids.size(0) == completion_mask.size(0) == advantages.size(0) == B * G, \\\n",
        "#        \"batch dimension mismatch\"\n",
        "\n",
        "# for row_m, row_c in zip(completion_mask, result[\"completion_ids\"]):\n",
        "#     # figure out how many *real* half‑moves preceded the first pad\n",
        "#     if 0 in row_c:\n",
        "#         L = (row_c != pad_id).nonzero(as_tuple=False)[-1].item() + 1\n",
        "#     else:\n",
        "#         L = row_c.size(0)             # no pad at all\n",
        "\n",
        "#     policy_bits   = row_m[:L:2]        # 0,2,4,...\n",
        "#     response_bits = row_m[1:L:2]       # 1,3,5,...\n",
        "\n",
        "#     assert policy_bits.bool().all(),      \"even half‑moves must be 1\"\n",
        "#     if response_bits.numel():             # may be empty if L is odd\n",
        "#         assert not response_bits.bool().any(), \"odd half‑moves must be 0\"\n",
        "\n",
        "\n",
        "# print(\"\\nAll tests passed ✅\")\n"
      ],
      "metadata": {
        "id": "lAIWPEe8BC03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GRPO.grpo_trainer import GRPOArgs, GRPOTrainer\n",
        "ref_model = AutoregressiveTransformer(ChessConfig()).to(device)\n",
        "trainer = GRPOTrainer(model, ref_model, GRPOArgs())"
      ],
      "metadata": {
        "id": "IBtm5fOBBz5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(d, batch_size=8, shuffle=True)\n",
        "trainer.train(train_loader, path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o395P4EIDM1I",
        "outputId": "cbb566de-3cd4-4797-a900-247372e716eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step      5 | loss   0.0091\n",
            "step     10 | loss   0.0097\n",
            "step     15 | loss   0.0081\n",
            "step     20 | loss   0.2043\n",
            "step     25 | loss   0.0085\n",
            "step     30 | loss   0.0090\n",
            "step     35 | loss   0.0095\n",
            "step     40 | loss   0.0094\n",
            "step     45 | loss   0.0114\n",
            "step     50 | loss   0.2876\n",
            "step     55 | loss   0.1387\n",
            "step     60 | loss   0.0094\n",
            "step     65 | loss   0.0107\n",
            "step     70 | loss   0.2189\n",
            "step     75 | loss   0.0098\n",
            "step     80 | loss   0.1781\n",
            "step     85 | loss   0.0080\n",
            "step     90 | loss   0.0091\n",
            "step     95 | loss   0.0100\n",
            "step    100 | loss   0.0105\n",
            "step    105 | loss   0.0090\n",
            "step    110 | loss   0.0100\n",
            "step    115 | loss   0.0087\n",
            "step    120 | loss   0.0108\n",
            "step    125 | loss   0.0085\n",
            "step    130 | loss   0.5018\n",
            "step    135 | loss   0.0093\n",
            "step    140 | loss   0.0279\n",
            "step    145 | loss   0.0112\n",
            "step    150 | loss   0.0097\n",
            "step    155 | loss   0.0101\n",
            "step    160 | loss   0.0105\n",
            "step    165 | loss   0.0097\n",
            "step    170 | loss   0.0095\n",
            "step    175 | loss   0.0098\n",
            "step    180 | loss   0.1086\n",
            "step    185 | loss   0.0093\n",
            "step    190 | loss   0.0097\n",
            "step    195 | loss   0.0095\n",
            "step    200 | loss   0.1508\n",
            "step    205 | loss   0.0094\n",
            "step    210 | loss   0.0096\n",
            "step    215 | loss   0.0094\n",
            "step    220 | loss   0.0103\n",
            "step    225 | loss   0.0107\n",
            "step    230 | loss   0.0085\n",
            "step    235 | loss   0.0094\n",
            "step    240 | loss   0.1180\n",
            "step    245 | loss   0.0103\n",
            "step    250 | loss   0.0109\n",
            "step    255 | loss   0.5522\n",
            "step    260 | loss   0.0090\n",
            "step    265 | loss   0.0096\n",
            "step    270 | loss   0.0110\n",
            "step    275 | loss   0.0105\n",
            "step    280 | loss   0.0109\n",
            "step    285 | loss   0.1088\n",
            "step    290 | loss   0.0115\n",
            "step    295 | loss   0.0101\n",
            "step    300 | loss   0.0096\n",
            "step    305 | loss   0.0114\n",
            "step    310 | loss   0.0104\n",
            "step    315 | loss   0.0104\n",
            "step    320 | loss   0.0107\n",
            "step    325 | loss   0.0120\n",
            "step    330 | loss   0.0090\n",
            "step    335 | loss   0.0095\n",
            "step    340 | loss   0.0099\n",
            "step    345 | loss   0.2529\n",
            "step    350 | loss   0.0098\n",
            "step    355 | loss   0.0099\n",
            "step    360 | loss   0.0108\n",
            "step    365 | loss   0.4243\n",
            "step    370 | loss   0.0716\n",
            "step    375 | loss   0.0110\n",
            "step    380 | loss   0.0100\n",
            "step    385 | loss   0.0102\n",
            "step    390 | loss   0.1881\n",
            "step    395 | loss   0.0092\n",
            "step    400 | loss   0.1511\n",
            "step    405 | loss   0.0109\n",
            "step    410 | loss   0.0099\n",
            "step    415 | loss  -0.0253\n",
            "step    420 | loss   0.0105\n",
            "step    425 | loss   0.0118\n",
            "step    430 | loss   0.0755\n",
            "step    435 | loss   0.0112\n",
            "step    440 | loss   0.0102\n",
            "step    445 | loss   0.2281\n",
            "step    450 | loss   0.0098\n",
            "step    455 | loss   0.0107\n",
            "step    460 | loss   0.3342\n",
            "step    465 | loss   0.1866\n",
            "step    470 | loss   0.0114\n",
            "step    475 | loss   0.0124\n",
            "step    480 | loss   0.0099\n",
            "step    485 | loss   0.5727\n",
            "step    490 | loss   0.4031\n",
            "step    495 | loss   0.0100\n",
            "step    500 | loss   0.0106\n",
            "step    505 | loss   0.0103\n",
            "step    510 | loss   0.0102\n",
            "step    515 | loss   0.0105\n",
            "step    520 | loss   0.0107\n",
            "step    525 | loss  -0.0538\n",
            "step    530 | loss   0.0123\n",
            "step    535 | loss   0.0119\n",
            "step    540 | loss   0.0095\n",
            "step    545 | loss   0.0135\n",
            "step    550 | loss   0.1091\n",
            "step    555 | loss   0.0140\n",
            "step    560 | loss   0.0120\n",
            "step    565 | loss   0.2170\n",
            "step    570 | loss   0.0114\n",
            "step    575 | loss   0.0137\n",
            "step    580 | loss   0.0132\n",
            "step    585 | loss   0.1503\n",
            "step    590 | loss   0.0139\n",
            "step    595 | loss   0.0102\n",
            "step    600 | loss   0.0106\n",
            "step    605 | loss   0.4352\n",
            "step    610 | loss   0.0120\n",
            "step    615 | loss   0.0134\n",
            "step    620 | loss   0.0144\n",
            "step    625 | loss   0.1106\n",
            "step    630 | loss   0.0136\n",
            "step    635 | loss   0.0130\n",
            "step    640 | loss   0.0133\n",
            "step    645 | loss   0.0111\n",
            "step    650 | loss   0.0114\n",
            "step    655 | loss   0.0840\n",
            "step    660 | loss   0.0248\n",
            "step    665 | loss   0.3342\n",
            "step    670 | loss   0.0126\n",
            "step    675 | loss   0.0123\n",
            "step    680 | loss   0.0121\n",
            "step    685 | loss   0.0130\n",
            "step    690 | loss   0.0145\n",
            "step    695 | loss   0.0842\n",
            "step    700 | loss   0.0121\n",
            "step    705 | loss   0.0127\n",
            "step    710 | loss   0.0112\n",
            "step    715 | loss   0.0131\n",
            "step    720 | loss   0.0132\n",
            "step    725 | loss   0.0122\n",
            "step    730 | loss   0.0124\n",
            "step    735 | loss   0.0113\n",
            "step    740 | loss   0.0118\n",
            "step    745 | loss   0.1537\n",
            "step    750 | loss   0.0121\n",
            "step    755 | loss   0.2066\n",
            "step    760 | loss   0.4052\n",
            "step    765 | loss   0.0122\n",
            "step    770 | loss   0.0116\n",
            "step    775 | loss   0.0107\n",
            "step    780 | loss   0.0128\n",
            "step    785 | loss   0.0131\n",
            "step    790 | loss   0.0142\n",
            "step    795 | loss   0.0118\n",
            "step    800 | loss   0.2783\n",
            "step    805 | loss   0.0162\n",
            "step    810 | loss   0.0128\n",
            "step    815 | loss   0.0134\n",
            "step    820 | loss   0.0120\n",
            "step    825 | loss   0.0106\n",
            "step    830 | loss   0.0124\n",
            "step    835 | loss   0.0139\n",
            "step    840 | loss   0.0135\n",
            "step    845 | loss   0.0098\n",
            "step    850 | loss   0.0103\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-5c62c71e2e51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/RL-Chess-Transformers/GRPO/grpo_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataloader, engine_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# rollout → batch dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 batch = self._generate_completions_and_score(\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RL-Chess-Transformers/GRPO/grpo_trainer.py\u001b[0m in \u001b[0;36m_generate_completions_and_score\u001b[0;34m(self, model, prompt_ids, engine_path, depth, num_generations, num_moves, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             base_eval = torch.tensor(\n\u001b[0;32m--> 213\u001b[0;31m                 [\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mpov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmate_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RL-Chess-Transformers/GRPO/grpo_trainer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    212\u001b[0m             base_eval = torch.tensor(\n\u001b[1;32m    213\u001b[0m                 [\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mpov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmate_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chess/engine.py\u001b[0m in \u001b[0;36manalyse\u001b[0;34m(self, board, limit, multipv, game, info, root_moves, options)\u001b[0m\n\u001b[1;32m   2992\u001b[0m                 self._timeout_for(limit))\n\u001b[1;32m   2993\u001b[0m             \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_coroutine_threadsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2994\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLimit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultipv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINFO_ALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_moves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfigMapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSimpleAnalysisResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer._metrics[\"train\"][\"clip_ratio/high_mean\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMyS8kr7He3Q",
        "outputId": "d7e730df-0129-4e11-f494-a0d374c05976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l24sQWdlD5g6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUGmCGrpkvfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}